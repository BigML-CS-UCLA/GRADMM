# GRADMM

This repository is the official implementation of our ICML 2025 paper [Synthetic Text Generation for Training Large Language Models via Gradient Matching](https://arxiv.org/pdf/2502.17607).

## ðŸ”— Quick Links
- [GRADMM](#gradmm)
  - [ðŸ”— Quick Links](#-quick-links)
  - [Install Requirements](#install-requirements)
  - [Data Preparation](#data-preparation)
  - [Data Generation Pipeline](#data-generation-pipeline)
  - [Evaluation](#evaluation)
  - [Bugs or Questions?](#bugs-or-questions)
  - [Citation](#citation)
  - [Acknowledgements](#acknowledgements)


## Install Requirements
```bash
Coming soon
```

## Data Preparation
Coming soon

## Data Generation Pipeline
```bash
Coming soon
```

## Evaluation
```bash
Coming soon
```

## Bugs or Questions?
If you have any questions related to the code or the paper, feel free to email Dang Nguyen (nguyentuanhaidang@gmail.com). If you encounter any problems when using the code, or want to report a bug, you can open an issue. Please try to specify the problem with details so we can help you better and quicker!

## Citation
Please cite our paper if you find the repo helpful in your work:

```bibtex
@article{nguyen2025synthetic,
  title={Synthetic Text Generation for Training Large Language Models via Gradient Matching},
  author={Nguyen*, Dang and Li*, Zeman and Bateni, Mohammadhossein and Mirrokni, Vahab and Razaviyayn, Meisam and Mirzasoleiman, Baharan},
  journal={International Conference on Machine Learning (ICML)},
  year={2025}
}
```

## Acknowledgements
The structure of this repository is largely based on the official implementation of [lamp](https://github.com/eth-sri/lamp) and [Addax](https://github.com/optimization-for-data-driven-science/Addax). We are grateful for their open sources.