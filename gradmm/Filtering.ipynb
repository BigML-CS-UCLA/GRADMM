{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BxXdLt4N0f6s"
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6555,
     "status": "ok",
     "timestamp": 1738278209844,
     "user": {
      "displayName": "Dang Nguyen",
      "userId": "05791393081131533369"
     },
     "user_tz": 480
    },
    "id": "nGSdefRd0mN7",
    "outputId": "5d21fc7a-bfb0-43d7-9ddc-74a1acb5ca2a"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from filtering import (\n",
    "    get_args,\n",
    "    load_model,\n",
    "    output_to_jsonl,\n",
    "    filtering,\n",
    "    load_real_data,\n",
    "    load_syn_data,\n",
    "    compute_average_grads,\n",
    "    calculate_recon_loss_ids,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZulhIga00Wb"
   },
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 53,
     "status": "ok",
     "timestamp": 1738278212664,
     "user": {
      "displayName": "Dang Nguyen",
      "userId": "05791393081131533369"
     },
     "user_tz": 480
    },
    "id": "nGFy0vmV03bw",
    "outputId": "1da8bf16-29c4-43cb-e647-8f4b0036e6c8"
   },
   "outputs": [],
   "source": [
    "json_file = 'synthetic_data'\n",
    "file_dir = './synthetic_data/'\n",
    "exp_pattern = 'test' # Change the pattern here\n",
    "training_dir = glob.glob(os.path.join(file_dir, f'{exp_pattern}*'))\n",
    "print(len(training_dir))\n",
    "\n",
    "input_flags = [sys.argv[0],\n",
    "               '--dataset', 'sst2', # sst2, rotten_tomatoes, TwitterEmotion\n",
    "               '--model_name', 'phi',\n",
    "               '--pos_label', 'positive', # positive\n",
    "               '--neg_label', 'negative', # negative\n",
    "               '--gen_bs', '10',\n",
    "               '--use_instruction', 'false',\n",
    "               '--use_fewshot', 'true',\n",
    "               '--filter_score', 'cls', # cls\n",
    "               '--filter_method', 'remove', # remove, relabel, top_score, first, bottom_score, greedy_selection\n",
    "               '--coeff_perplexity', '0', # 0, 0.05\n",
    "               '--top_n', '50',\n",
    "               '--file_dir', file_dir,\n",
    "               '--json_file', json_file,\n",
    "               '--clean', 'true',\n",
    "               '--balance_score', 'true',\n",
    "               '--per_label', 'true',\n",
    "               '--interleave_label', 'false',\n",
    "]\n",
    "sys.argv = input_flags\n",
    "\n",
    "# Load parameters\n",
    "args = get_args()\n",
    "for arg in vars(args):\n",
    "    print(f\"{arg}: {getattr(args, arg)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7u6NLNxuDfy0"
   },
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 41578,
     "status": "ok",
     "timestamp": 1738278255924,
     "user": {
      "displayName": "Dang Nguyen",
      "userId": "05791393081131533369"
     },
     "user_tz": 480
    },
    "id": "tBOyZDqh4CIV",
    "outputId": "b3eed4bc-c7ee-43ba-d828-d178de757c99"
   },
   "outputs": [],
   "source": [
    "tokenizer, model, device = load_model(args.model_name)\n",
    "\n",
    "# Set last layer gradient\n",
    "LAST_LAYERS = [\"lm_head\"] \n",
    "\n",
    "named_parameters_to_optim = []\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if any(substring in name for substring in LAST_LAYERS):\n",
    "        named_parameters_to_optim.append((name, param))\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "assert len(named_parameters_to_optim) != 0, \"no layer found\"\n",
    "print(f\"Set gradients for {len(named_parameters_to_optim)} layers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering - Clean remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set file_path\n",
    "args.filter_method = 'remove'\n",
    "args.gen_bs = 10 # Make sure it matches the setting\n",
    "\n",
    "# Set real_id\n",
    "for run in tqdm(training_dir):\n",
    "    file_path = os.path.join(run, f'{args.json_file}.jsonl')\n",
    "    samples = []\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            samples.append(json.loads(line))\n",
    "    \n",
    "    with open(file_path, 'w') as f:\n",
    "        for sample in samples:\n",
    "            sample[\"real_id\"] = sample[\"id\"] // args.gen_bs\n",
    "            f.write(json.dumps(sample) + \"\\n\")\n",
    "\n",
    "# Clean remove\n",
    "filtering(\n",
    "    args,\n",
    "    training_dir,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2K503ycBEKCP"
   },
   "source": [
    "# (Re)calculate rec_loss_ids per sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load real data & compute grad\n",
    "pos_sequences, neg_sequences, pos_labels, neg_labels = load_real_data(\n",
    "    dataset_name='sst2',\n",
    "    split='validation',\n",
    "    device=device,\n",
    "    n_gen_samples=100,\n",
    "    n_fewshot=0,\n",
    "    random_seed=42,\n",
    "    subset=20,\n",
    ")\n",
    "\n",
    "print(pos_sequences[:5])\n",
    "\n",
    "real_pos_grads = compute_average_grads(\n",
    "    args,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    pos_sequences,\n",
    "    pos_labels\n",
    ")\n",
    "\n",
    "real_neg_grads = compute_average_grads(\n",
    "    args,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    neg_sequences,\n",
    "    neg_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 594553,
     "status": "ok",
     "timestamp": 1738279924887,
     "user": {
      "displayName": "Dang Nguyen",
      "userId": "05791393081131533369"
     },
     "user_tz": 480
    },
    "id": "x9_YnOLOFcjz",
    "outputId": "803b7313-0e74-47e4-d0d6-7cb4ab7d3467"
   },
   "outputs": [],
   "source": [
    "if args.dataset in ['sst2', 'rotten_tomatoes']:\n",
    "    POS_LABEL = 'great'\n",
    "    NEG_LABEL = 'bad'\n",
    "elif args.dataset == 'TwitterEmotion':\n",
    "    POS_LABEL = 'joy'\n",
    "    NEG_LABEL = 'sadness'\n",
    "\n",
    "for run in tqdm(training_dir):\n",
    "    syn_data_path = os.path.join(run, f'synthetic_data_clean_remove_cls_phi_{args.dataset}_{args.pos_label}_{args.neg_label}_instrFalse_fsTrue.jsonl')\n",
    "    if not os.path.exists(syn_data_path):\n",
    "        print(syn_data_path)\n",
    "        continue\n",
    "\n",
    "    # Load synthetic data\n",
    "    syn_pos_sequences, syn_neg_sequences = load_syn_data(str(syn_data_path), args.dataset)\n",
    "\n",
    "    list_raw_pos_loss = calculate_recon_loss_ids(\n",
    "        syn_pos_sequences,\n",
    "        [POS_LABEL for _ in range(len(syn_pos_sequences))],\n",
    "        real_pos_grads,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        dataset=args.dataset\n",
    "    )\n",
    "\n",
    "    list_raw_neg_loss = calculate_recon_loss_ids(\n",
    "        syn_neg_sequences,\n",
    "        [NEG_LABEL for _ in range(len(syn_neg_sequences))],\n",
    "        real_neg_grads,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        dataset=args.dataset\n",
    "    )\n",
    "\n",
    "    # Read the JSONL file\n",
    "    samples = []\n",
    "    with open(syn_data_path, 'r') as file:\n",
    "        for line in file:\n",
    "            sample = json.loads(line)\n",
    "            samples.append(sample)\n",
    "\n",
    "    # Group samples by label\n",
    "    grouped_samples = defaultdict(list)\n",
    "    for sample in samples:\n",
    "        grouped_samples[sample['label']].append(sample)\n",
    "\n",
    "    loss_dict = {\n",
    "        1: list_raw_pos_loss,\n",
    "        0: list_raw_neg_loss\n",
    "    }\n",
    "    list_out_samples = []\n",
    "\n",
    "    for label, label_samples in grouped_samples.items():\n",
    "        for sample_idx, sample in enumerate(label_samples):\n",
    "            sample['rec_loss_ids'] = loss_dict[label][sample_idx]\n",
    "            list_out_samples.append(sample)\n",
    "\n",
    "    output_to_jsonl(args, list_out_samples, syn_data_path, post_processing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WqQ9VwDQRmEf"
   },
   "source": [
    "# Extract top *score*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 266879,
     "status": "ok",
     "timestamp": 1738280192276,
     "user": {
      "displayName": "Dang Nguyen",
      "userId": "05791393081131533369"
     },
     "user_tz": 480
    },
    "id": "sX2fIke9KVyg",
    "outputId": "c4bae617-ff0e-4b78-a262-a0c182fd7009"
   },
   "outputs": [],
   "source": [
    "args.filter_method = 'top_score'\n",
    "args.coeff_perplexity = 0\n",
    "args.json_file = f'synthetic_data_clean_remove_cls_phi_{args.dataset}_{args.pos_label}_{args.neg_label}_instrFalse_fsTrue'\n",
    "args.top_n = 50 # Modify here for different budget\n",
    "args.balance_score = True\n",
    "\n",
    "filtering(\n",
    "    args,\n",
    "    training_dir,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    device,\n",
    "    num_out=5 if args.top_n == 3 else None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 237861,
     "status": "ok",
     "timestamp": 1738280430560,
     "user": {
      "displayName": "Dang Nguyen",
      "userId": "05791393081131533369"
     },
     "user_tz": 480
    },
    "id": "ssP9FwM3LgFv",
    "outputId": "49218a87-b444-428f-a336-a8174ecd1550"
   },
   "outputs": [],
   "source": [
    "args.coeff_perplexity = 0.05\n",
    "\n",
    "filtering(\n",
    "    args,\n",
    "    training_dir,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    device,\n",
    "    num_out=5 if args.top_n == 3 else None\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "last_runtime": {
    "build_target": "//experimental/users/zemanli/data_generation/colab:notebook",
    "kind": "private"
   },
   "provenance": [
    {
     "file_id": "1er0jWZX-WUTUbBa8QJo92PYhHTNakTvP",
     "timestamp": 1743102790262
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "gradmm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
